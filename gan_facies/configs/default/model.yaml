--- !model
architecture: sagan  # sagan or cond_sagan
data_size: 64  # could be either 32, 64, 128 or 256
init_method: 'default'  # could be either 'default', 'orthogonal', 'normal', or 'glorot'
# attn_layer_num: number of the block(s) where attention is applied to the output (starting from 1)
# NOTE: the number of blocks depends on the data size only:
# data_size 32: 4 blocks, data_size 64: 5 blocks, data_size 128: 6 blocks, data_size 256: 7 blocks
attn_layer_num: [3, 4]
sym_attn: True # if True, the attention layer indexes are n_blocks - attn_layer_num for the discriminator, otherwise attn_layer_num
z_dim: 128
g_conv_dim: 64  # number of channels before the last layer of the generator
d_conv_dim: 64  # number of channels after the first layer of the discriminator
cond_dim_ratio: -1 # no used here (only for conditional models)

attention: !attention
  n_heads: 1  # number of attention heads
  out_layer: True  # whether to apply a linear layer to the output of the attention
  qk_ratio: 8  # dimension for queries and keys is input_dim // qk_ratio
  v_ratio: 2  # dimension for values is input_dim // qk_ratio, should be 1 if out_layer=False
